# Recent Advances in Vision Pre-training

If you want to contribute this repository, please fork and revise, and create a pull request for a update. Alternatively, you can contact Xiaoshui Huang[huangxiaoshui@pjlab.org.cn] to do the update. 

## Table of Contents

* [Leaning paradigm](#leaning-paradigm)
* [Model design](#model-design)
* [Fine tune](#Fine-tune)

 
# Leaning paradigm

2022

[Versatile Multi-Modal Pre-Training for Human-Centric Perception](https://arxiv.org/pdf/2203.13815.pdf) CVPR 2022, [***]

[CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding](https://arxiv.org/pdf/2203.00680.pdf) CVPR 2022, [**]

[PointCLIP: Point Cloud Understanding by CLIP](https://arxiv.org/abs/2112.02413) CVPR 2022, [**]

[PoseTriplet: Co-evolving 3D Human Pose Estimation, Imitation, and Hallucination under Self-supervision](https://arxiv.org/pdf/2203.15625.pdf) CVPR 2022, [**]

[Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling](https://arxiv.org/abs/2111.14819) CVPR 2022, [**]

[Masked Discrimination for Self-Supervised Learning on Point Clouds](https://arxiv.org/pdf/2203.11183.pdf) arXiv 2022, [**]

[Masked Autoencoders for Point Cloud Self-supervised Learning](https://arxiv.org/pdf/2203.06604.pdf) arXiv 2022, [*]

[Implicit Autoencoder for Point Cloud Self-supervised Representation Learning](https://arxiv.org/abs/2201.00785) arXiv 2022, [**]

[RotoGrad: Gradient Homogenization in Multitask Learning](https://arxiv.org/pdf/2103.02631.pdf) ICLR 2022, [**]

[X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation](https://arxiv.org/pdf/2203.08764.pdf) arXiv 2022, [**]

[A Unified Query-based Paradigm for Point Cloud Understanding](https://arxiv.org/pdf/2203.01252.pdf) arXiv 2022, [**]

2021

[Unsupervised Point Cloud Pre-Training via Occlusion Completion](https://arxiv.org/abs/2010.01089) ICCV 2021, [**]

[Contrastive Multimodal Fusion with TupleInfoNCE](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Contrastive_Multimodal_Fusion_With_TupleInfoNCE_ICCV_2021_paper.pdf) ICCV 2021, [**]

[Pri3D: Can 3D Priors Help 2D Representation Learning?](https://arxiv.org/pdf/2104.11225.pdf) ICCV 2021, [**]

[Learning Transferable Features for Point Cloud Detection via 3D Contrastive Co-training](https://proceedings.neurips.cc/paper/2021/file/b3b25a26a0828ea5d48d8f8aa0d6f9af-Paper.pdf) NeurIPS 2021, [*]

 
2020

[P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for RGB-D Scene Understanding](https://arxiv.org/pdf/2012.13089.pdf) arXiv 2020, [*]

[PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding](https://arxiv.org/abs/2007.10985) ECCV 2020, [**]


2019

[Self-Supervised Deep Learning on Point Clouds by Reconstructing Space](https://arxiv.org/abs/1901.08396) NuerIPS 2019, [**]

2018

[Representation Learning with Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf) NeurIPS 2018, [***]

[Learning Representations and Generative Models for 3D Point Clouds](https://arxiv.org/pdf/1707.02392.pdf)  ICML 2018, [*]



# Model design

2022

[Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) CVPR 2022, [**]

[MetaFormer is Actually What You Need for Vision](https://arxiv.org/pdf/2111.11418.pdf) CVPR 2022, [**]

2021

[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) ICCV 2021, [***]

[MLP-Mixer: An all-MLP Architecture for Vision](https://proceedings.neurips.cc//paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf) NeurIPS 2021, [***]

[CoAtNet: Marrying Convolution and Attention for All Data Sizes](https://arxiv.org/abs/2106.04803) NeurIPS 2021, [***]


# Fine tune

2022

[Visual Prompt Tuning](https://arxiv.org/pdf/2203.12119.pdf) arXiv 2022, [**]

